{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9ec3ce-c47c-4b8e-a2e5-391e3a6b3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, classification_report, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db38842-e641-4bc0-9051-f9aa16bac474",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_visit = pd.read_csv('NPHA-doctor-visits.csv')\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "543eea06-3dbe-4da0-ab17-6c7ed3490e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Age column since it has only one unique value (2)\n",
    " # print(doc_visit.columns)\n",
    "doc_visit.drop(['Age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "880d1078-df06-43da-954e-2f0701cc06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace only -1 with None (NaN)\n",
    "doc_visit.replace(-1, pd.NA, inplace=True)\n",
    "\n",
    "#Check if the replacement worked\n",
    "# print(doc_visit.isnull().sum())  # This will give you the count of NaN values per column(values are now empty there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11df6971-fdc4-4ee4-ab66-750cffa17e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Optionally, Impute the missing values (NaN) with the mode or most frequent value\n",
    "# For this example, we'll impute with the most frequent value for each column.\n",
    "doc_visit.fillna(doc_visit.mode().iloc[0], inplace=True)\n",
    "doc_visit = doc_visit.infer_objects(copy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52bd7872-cfae-4374-9d7a-d549d3c3adde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714, 13)\n",
      "(714,)\n"
     ]
    }
   ],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = doc_visit.drop(\"Number of Doctors Visited\", axis=1)  # Features\n",
    "y = doc_visit[\"Number of Doctors Visited\"]  # Target\n",
    "\n",
    "# Check the shape of X and y to ensure they are consistent\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "767db70c-5423-4cb4-8b13-21d0e6f5efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3983020-a0b2-40e8-910a-03c1865fb192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE: Counter({3: 298, 2: 298, 1: 298})\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_train_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a87ae2d4-a09d-4b24-b7b7-a315b14bba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "266a5f1e-957f-4c8b-b333-fa7ace2e543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(\"Confusion Matrix:\\n\", matrix)\n",
    "    print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb14bfe2-fb6d-4f93-a611-9a165ac02cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ LogisticRegression\n",
      "Average accuracy: 0.4358569828230022\n",
      "Confusion Matrix:\n",
      " [[ 5 15  8]\n",
      " [19 26 29]\n",
      " [ 7 15 19]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.16      0.18      0.17        28\n",
      "           2       0.46      0.35      0.40        74\n",
      "           3       0.34      0.46      0.39        41\n",
      "\n",
      "    accuracy                           0.35       143\n",
      "   macro avg       0.32      0.33      0.32       143\n",
      "weighted avg       0.37      0.35      0.35       143\n",
      "\n",
      "ðŸš€ Random Forest\n",
      "Average accuracy: 0.5939973861090365\n",
      "Confusion Matrix:\n",
      " [[ 5 13 10]\n",
      " [10 44 20]\n",
      " [ 5 20 16]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.18      0.21        28\n",
      "           2       0.57      0.59      0.58        74\n",
      "           3       0.35      0.39      0.37        41\n",
      "\n",
      "    accuracy                           0.45       143\n",
      "   macro avg       0.39      0.39      0.39       143\n",
      "weighted avg       0.44      0.45      0.45       143\n",
      "\n",
      "ðŸš€ SVC\n",
      "Average accuracy: 0.431908140403286\n",
      "Confusion Matrix:\n",
      " [[ 5 17  6]\n",
      " [23 28 23]\n",
      " [11 13 17]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.13      0.18      0.15        28\n",
      "           2       0.48      0.38      0.42        74\n",
      "           3       0.37      0.41      0.39        41\n",
      "\n",
      "    accuracy                           0.35       143\n",
      "   macro avg       0.33      0.32      0.32       143\n",
      "weighted avg       0.38      0.35      0.36       143\n",
      "\n",
      "ðŸš€ DecisionTree\n",
      "Average accuracy: 0.5342886482449589\n",
      "Confusion Matrix:\n",
      " [[ 7  9 12]\n",
      " [18 41 15]\n",
      " [15 18  8]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.17      0.25      0.21        28\n",
      "           2       0.60      0.55      0.58        74\n",
      "           3       0.23      0.20      0.21        41\n",
      "\n",
      "    accuracy                           0.39       143\n",
      "   macro avg       0.34      0.33      0.33       143\n",
      "weighted avg       0.41      0.39      0.40       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_scaled = np.vstack((X_train_scaled, X_test_scaled))\n",
    "y = np.concatenate((y_train_resampled, y_test))\n",
    "\n",
    "# Initializing Models\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVC\": SVC(kernel ='linear'),\n",
    "    \"DecisionTree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "# Producing cross validation score for the models\n",
    "for model_name in models:\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Evaluate the model's accuracy using cross-validation\n",
    "    accuracies = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    print(\"ðŸš€\", model_name)\n",
    "    print(\"Average accuracy:\", np.mean(accuracies))\n",
    "\n",
    "    model.fit(X_train_scaled, y_train_resampled)\n",
    "    evaluate(model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ee7c98a-9bbd-444c-89cf-a63fe0036c4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, cross_val_score\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder, MinMaxScaler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "doc_visit = pd.read_csv(\"NPHA-doctor-visits.csv\")\n",
    "doc_visit.replace(-1, np.nan, inplace=True)  # Replace invalid values\n",
    "doc_visit.fillna(doc_visit.mode().iloc[0], inplace=True)  # Impute missing values\n",
    "\n",
    "# Encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(doc_visit['Number of Doctors Visited'].values.reshape(-1, 1))\n",
    "\n",
    "# Drop target column\n",
    "X = doc_visit.drop('Number of Doctors Visited', axis=1)\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to build the neural network model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=X_train.shape[1], activation='relu'),  # Hidden layer\n",
    "        Dropout(0.2),  # Dropout for regularization\n",
    "        Dense(32, activation='relu'),  # Another hidden layer\n",
    "        Dense(3, activation='softmax')  # Output layer for 3 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model in KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "# Perform cross-validation to evaluate the model\n",
    "cross_val_scores = cross_val_score(model, X_scaled, y_encoded, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {cross_val_scores}\")\n",
    "print(f\"Mean cross-validation score: {cross_val_scores.mean()}\")\n",
    "\n",
    "# Train the model on the training data\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "y_test_classes = y_test.argmax(axis=1)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_classes, y_pred_classes))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=encoder.categories_[0], yticklabels=encoder.categories_[0])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Optionally, visualize the training and validation accuracy over epochs\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20135ca8-6dc7-46be-b5f1-877d729f973e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
